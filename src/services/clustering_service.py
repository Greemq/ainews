from typing import List, Dict
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
import numpy as np
import hdbscan
from collections import Counter
import json

from src.services.gpt_service import GPTservice
from src.models.news import News
from sqlalchemy import text


class ClusteringService:
    def __init__(self, mysql_db: Session, pg_db: Session):
        """
        :param mysql_db: —Å–µ—Å—Å–∏—è MySQL (–æ—Å–Ω–æ–≤–Ω–∞—è –±–∞–∑–∞ —Å–æ —Å—Ç–∞—Ç—å—è–º–∏)
        :param pg_db: —Å–µ—Å—Å–∏—è Postgres (–¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤)
        """
        self.mysql_db = mysql_db
        self.pg_db = pg_db
        self.gpt = GPTservice()

    # ============================
    #   –≠–ú–ë–ï–î–î–ò–ù–ì–ò
    # ============================
    def fetch_recent_news(self, hours: int = 24) -> List[News]:
        cutoff = datetime.utcnow() - timedelta(hours=hours)
        return (
            self.mysql_db.query(News)
            .filter(News.published_at >= cutoff)
            .filter(News.has_summary.is_(True))
            .all()
        )

    def embedding_exists(self, news_id: int) -> bool:
        sql = text("SELECT 1 FROM news_embeddings WHERE news_id = :news_id")
        row = self.pg_db.execute(sql, {"news_id": news_id}).fetchone()
        return row is not None

    def save_embedding(self, news_id: int, title: str, summary: str, embedding: List[float]):
        insert_sql = text("""
            INSERT INTO news_embeddings (news_id, title, summary, embedding, created_at)
            VALUES (:news_id, :title, :summary, :embedding, NOW())
        """)
        self.pg_db.execute(insert_sql, {
            "news_id": news_id,
            "title": title,
            "summary": summary,
            "embedding": embedding
        })
        self.pg_db.commit()
        print(f"[OK] Embedding —Å–æ—Ö—Ä–∞–Ω—ë–Ω –¥–ª—è news_id={news_id}")

    def process_recent_news(self, hours: int = 24):
        articles = self.fetch_recent_news(hours=hours)
        for art in articles:
            try:
                if self.embedding_exists(art.id):
                    print(f"[SKIP] Embedding —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–ª—è news_id={art.id}")
                    continue

                text_for_emb = art.summary_ru or art.title
                embedding = self.gpt.get_embedding(text_for_emb)
                self.save_embedding(art.id, art.title, text_for_emb, embedding)

            except Exception as e:
                print(f"[ERR] news_id={art.id}: {e}")

    # ============================
    #   –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø
    # ============================
    def fetch_embeddings_with_summaries(self, hours: int = 24):
        cutoff = datetime.utcnow() - timedelta(hours=hours)
        sql = text("""
            SELECT news_id, title, summary, embedding
            FROM news_embeddings
            WHERE created_at >= :cutoff
        """)
        rows = self.pg_db.execute(sql, {"cutoff": cutoff}).fetchall()
        if not rows:
            return [], np.array([]), []

        ids, vectors, articles_info = [], [], []
        for r in rows:
            ids.append(r.news_id)
            articles_info.append({
                "id": r.news_id,
                "title": r.title,
                "summary": r.summary
            })
            if isinstance(r.embedding, str):
                vec = np.fromstring(r.embedding.strip("[]"), sep=",")
            else:
                vec = np.array(r.embedding, dtype=float)
            vectors.append(vec)

        return ids, np.vstack(vectors), articles_info

    def validate_cluster_with_gpt(self, cluster_label: int, articles: List[Dict]) -> List[Dict]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–¥–∏–Ω HDBSCAN-–∫–ª–∞—Å—Ç–µ—Ä –≤ GPT,
        –º–æ–¥–µ–ª—å –≤–Ω—É—Ç—Ä–∏ –º–æ–∂–µ—Ç –≤—ã–¥–µ–ª–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ç–µ–º.
        :return: —Å–ø–∏—Å–æ–∫ –º–∏–Ω–∏-–∫–ª–∞—Å—Ç–µ—Ä–æ–≤ [{"theme": ..., "article_ids": [...]}, ...]
        """
        if not articles or len(articles) < 3:
            return []

        cluster_data = [
            {
                "id": art["id"],
                "title": art["title"],
                "summary": art["summary"][:200] + "..." if len(art["summary"]) > 200 else art["summary"]
            }
            for art in articles
        ]

        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏ —Ä–∞–∑–±–µ–π –∏—Ö –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —Å–æ–±—ã—Ç–∏—è–º.

–°—Ç–∞—Ç—å–∏:
{json.dumps(cluster_data, ensure_ascii=False, indent=2)}

–ü—Ä–∞–≤–∏–ª–∞:
1. ...
5. –ï—Å–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≥—Ä—É–ø–ø –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π JSON: {{}}

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –°–¢–†–û–ì–û JSON:
{{
  "clusters": [
    {{
      "theme": "–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –≤–∏–∑–∏—Ç –¢–æ–∫–∞–µ–≤–∞ –≤ –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω",
      "article_ids": [111, 112]
    }}
  ]
}}
"""

        try:
            response = self.gpt.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_completion_tokens=600
            )
            result_text = response.choices[0].message.content.strip()

            print(f"===== GPT RAW RESPONSE (cluster {cluster_label}) =====")
            print(result_text)
            print("=====================================================")

            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].strip()

            try:
                result = json.loads(result_text)
            except json.JSONDecodeError as e:
                print(f"[WARN] JSON parse error (cluster {cluster_label}): {e}")
                return []

            clusters = []
            for cl in result.get("clusters", []):
                if len(cl.get("article_ids", [])) >= 3:
                    clusters.append({
                        "theme": cl["theme"],
                        "article_ids": cl["article_ids"]
                    })
            return clusters

        except Exception as e:
            print(f"[WARN] –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_label} —á–µ—Ä–µ–∑ GPT: {e}")
            return []

    def run_clustering(self, hours: int = 24, min_cluster_size: int = 3, min_samples: int = 2):
        news_ids, vectors, articles_info = self.fetch_embeddings_with_summaries(hours=hours)
        if len(news_ids) < min_cluster_size:
            print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ({len(news_ids)} < {min_cluster_size})")
            return

        print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é {len(news_ids)} —Å—Ç–∞—Ç–µ–π...")

        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric="euclidean",
            cluster_selection_epsilon=0.3
        )
        labels = clusterer.fit_predict(vectors)

        cluster_articles, label_counts = {}, Counter(labels)
        for label in label_counts:
            if label == -1:
                continue
            cluster_mask = labels == label
            cluster_arts = [articles_info[i] for i, mask in enumerate(cluster_mask) if mask]
            if len(cluster_arts) >= min_cluster_size:
                cluster_articles[label] = cluster_arts

        noise_count = label_counts.get(-1, 0)
        print(f"üìä HDBSCAN —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(cluster_articles)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏, {noise_count} —à—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫")

        if not cluster_articles:
            print("‚úÖ –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            return

        saved_clusters = 0
        for label, articles in cluster_articles.items():
            print(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä {label} –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é –≤ GPT...")
            validated = self.validate_cluster_with_gpt(label, articles)

            if not validated:
                print(f"‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä {label} –Ω–µ –¥–∞–ª –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ–¥—Ç–µ–º")
                continue

            for idx, cluster_data in enumerate(validated):
                article_ids, theme = cluster_data["article_ids"], cluster_data["theme"]

                insert_cluster_sql = text("""
                    INSERT INTO news_clusters (label, theme)
                    VALUES (:label, :theme)
                    RETURNING cluster_id
                """)
                res = self.pg_db.execute(insert_cluster_sql, {
                    "label": f"gpt_validated_{datetime.now().strftime('%Y%m%d_%H%M')}_{label}_{idx}",
                    "theme": theme
                })
                cluster_id = res.scalar()

                for news_id in article_ids:
                    insert_item_sql = text("""
                        INSERT INTO news_cluster_items (cluster_id, news_id)
                        VALUES (:cluster_id, :news_id)
                    """)
                    self.pg_db.execute(insert_item_sql, {
                        "cluster_id": cluster_id,
                        "news_id": news_id
                    })

                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–ª–∞—Å—Ç–µ—Ä {cluster_id}: {len(article_ids)} —Å—Ç–∞—Ç–µ–π")
                print(f"   üìù –¢–µ–º–∞: {theme}")
                saved_clusters += 1

        self.pg_db.commit()
        print(f"üéØ –ò—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_clusters} GPT-–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
